# 2023

## 2023-08-19

### logistic-regression 404

http://localhost:3000/logistic-regression

This used to work, I'm sure, but not sure how.  We needed a logistic-regression module to wire into the app module, then the route works.

### TypeError: Cannot read properties of undefined (reading 'match')

at TrendsDataService.preprocessData (C:\Users\timof\repos\node\google-trend-predictor\src\predictions\trends-data.service.ts:87:14)

```js
    for (const dataItem of dataset) {
      if (typeof dataItem?.isMajorTrend !== 'undefined' && dataItem?.isMajorTrend !== null) {
        const label = dataItem.isMajorTrend ? 1 : 0;
        if (typeof dataItem?.sparkline !== 'undefined' && dataItem?.sparkline !== null) {
          const sparklineValues = dataItem.sparkline
            .match(/\d+/g)
            ?.slice(0, 2) // Use only the first two values as features
            .map(Number) || [];
          features.push(sparklineValues);
          labels.push(label);
        }
      }
    }
```

The comment: *Use only the first two values as features* is a little strange.

Why isn't all of the sparkline data being used in training the logistics regression model?

ChatGPT
The comment "// Use only the first two values as features" indicates that only the first two values from the sparkline data are being used as features for training the logistic regression model. This approach is a design choice and might be influenced by several factors:

Dimensionality Reduction: Using all values from the sparkline data as features could result in a high-dimensional input space, which might lead to overfitting and increased computational complexity. By using only the first two values, the model focuses on the most relevant information and reduces the dimensionality of the feature space.

Simplicity: Logistic regression is a linear model and is most effective when dealing with relatively simple relationships between features and labels. Using a subset of the sparkline data simplifies the model and could prevent it from capturing noise or unnecessary complexity.

Interpretability: Logistic regression models are often used when you want to understand the relationship between the input features and the output (binary classification in this case). By using a subset of the sparkline data, the resulting model might be more interpretable, making it easier to understand how the selected features contribute to the classification decision.

Data Distribution: It's possible that, after analyzing the data, it was determined that the first two values of the sparkline data contain the most relevant information for predicting whether an item is a major trend or not.

It's important to note that feature selection and engineering are critical steps in machine learning, and they can significantly impact the performance and interpretability of the model. Depending on the specific problem and dataset, different feature selection strategies might be more appropriate.

If you have reason to believe that using all the values from the sparkline data would improve your model's performance, you can certainly modify the code to include all values instead of just the first two. However, be cautious of potential overfitting and increased computational demands, especially if your dataset is large.

User

I feel it is crucial to use all the sparkline data.  The other data in the trend is not important to predicting from a time series when there is a "hocky stick" like graph with incremental growth potential.  Please show the code to include all the sparkline data.   Also, does each sparkline data have to have the same number of time-value numbers?

```js
    for (const dataItem of dataset) {
      if (
        typeof dataItem?.isMajorTrend !== 'undefined' &&
        dataItem?.isMajorTrend !== null
      ) {
        const label = dataItem.isMajorTrend ? 1 : 0;
        if (
          typeof dataItem?.sparkline !== 'undefined' &&
          dataItem?.sparkline !== null
        ) {
          const sparklineValues = dataItem.sparkline.match(/\d+/g).map(Number); // Use all values from sparkline as features
          features.push(sparklineValues);
          labels.push(label);
        }
      }
    }
```

features 2780
labels 2780
[Nest] 28200  - 19/08/2023, 10:29:19 am   ERROR [ExceptionsHandler] input expected a batch of elements where each example has shape [2] (i.e.,tensor shape [*,2]) but the input received an input with 2780 examples, each with shape [70] (tensor shape [2780,70])
Error: input expected a batch of elements where each example has shape [2] (i.e.,tensor shape [*,2]) but the input received an input with 2780 examples, each with shape [70] (tensor shape [2780,70])
    at new ValueError (C:\Users\timof\repos\node\google-trend-predictor\node_modules\@tensorflow\tfjs\tfjs-layers\src\errors.ts:48:5)
    at standardizeInputData (C:\Users\timof\repos\node\google-trend-predictor\node_modules\@tensorflow\tfjs\tfjs-layers\src\engine\training.ts:170:17)
    at LayersModel.standardizeUserDataXY (C:\Users\timof\repos\node\google-trend-predictor\node_modules\@tensorflow\tfjs\tfjs-layers\src\engine\training.ts:1167:9)
    at LayersModel.<anonymous> (C:\Users\timof\repos\node\google-trend-predictor\node_modules\@tensorflow\tfjs\tfjs-layers\src\engine\training.ts:1194:14)
    at step (C:\Users\timof\repos\node\google-trend-predictor\node_modules\@tensorflow\tfjs\node_modules\tslib\tslib.es6.js:102:23)
    at Object.next (C:\Users\timof\repos\node\google-trend-predictor\node_modules\@tensorflow\tfjs\node_modules\tslib\tslib.es6.js:83:53)
    at C:\Users\timof\repos\node\google-trend-predictor\node_modules\@tensorflow\tfjs\node_modules\tslib\tslib.es6.js:76:71

The error you are encountering indicates a mismatch between the input shape expected by your logistic regression model and the shape of the data you are providing for training. The model is expecting input samples of shape [*, 2], but the input data you have provided has a shape of [2780, 70].

It seems that the sparkline data you are using for training has 70 values per example, but the model is designed to work with input samples of shape [2]. This could be because the sparkline data has not been properly processed before being used as input to the model.

Make sure that the sparkline data is correctly preprocessed to have a consistent shape of [2] for each example before feeding it to the model. If the sparkline data inherently has more than 2 values per example, you might need to rethink how you want to represent this data for training the logistic regression model.




User
We need to use the shape of [2780, 79].  How do I make that change to match the data we want to use?
ChatGPT
If you want to use the shape [2780, 79] for your input data, you need to adjust both your preprocessing and the model's architecture to accommodate this shape.

Here's how you can modify your code:

Preprocessing Data:
In your TrendsDataService class, adjust the preprocessData function to handle the new input shape:

typescript
Copy code
preprocessData(dataset: any[]): { features: number[][]; labels: number[] } {
  const features: number[][] = [];
  const labels: number[] = [];
  for (const dataItem of dataset) {
    if (
      typeof dataItem?.isMajorTrend !== 'undefined' &&
      dataItem?.isMajorTrend !== null
    ) {
      const label = dataItem.isMajorTrend ? 1 : 0;
      if (
        typeof dataItem?.sparkline !== 'undefined' &&
        dataItem?.sparkline !== null
      ) {
        const sparklineValues = dataItem.sparkline
          .match(/\d+/g)
          .map(Number); // Use all values from sparkline as features
        features.push(sparklineValues);
        labels.push(label);
      }
    }
  }
  console.log('features', features.length);
  console.log('labels', labels.length);
  return { features, labels };
}
Model Architecture:
Update the model architecture to handle the input shape [2780, 79]. You'll need to adjust the input shape of the first layer to match the new feature dimensions:

typescript
Copy code
this.model = tf.sequential();
this.model.add(
  tf.layers.dense({ units: 1, inputShape: [79], activation: 'sigmoid' }),
);
const learningRate = 0.1;
const optimizer = tf.train.sgd(learningRate);
this.model.compile({ optimizer, loss: 'binaryCrossentropy' });
Remember to update any parts of your code that depend on this input shape, including the training and prediction steps.

Please note that using a neural network architecture like a dense layer with sigmoid activation for time series data might not be the most appropriate choice. Neural networks are generally more suitable for tasks like image recognition or natural language processing. For time series data, you might want to consider other algorithms like LSTM or other time series-specific techniques. The choice of architecture depends on the characteristics of your data and the problem you are trying to solve.
